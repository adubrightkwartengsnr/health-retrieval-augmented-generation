# ğŸ©º Ask Your DigiDoctor ğŸ‘¨â€âš•ï¸

An AI-powered health assistant that allows you to ask questions about **stroke** using information extracted from research papers. Built with [Streamlit](https://streamlit.io/), [LangChain](https://www.langchain.com/), [FAISS](https://github.com/facebookresearch/faiss), [HuggingFace Transformers](https://huggingface.co/), and [Groqâ€™s LLaMA 3](https://groq.com/).

---

## ğŸš€ Features

- ğŸ§  Conversational AI using Groq's **LLaMA 3 70B** model
- ğŸ“š Uploads and processes multiple PDFs using **LangChain document loaders**
- ğŸ“ Stores document chunks in a **FAISS vector database**
- ğŸ” Retrieves relevant answers using **semantic search**
- ğŸ’¬ Remembers conversation history with **buffered memory**
- âš¡ Clean Streamlit interface for real-time Q&A

---

## ğŸ“ File Structure
â”œâ”€â”€ app.py # Main Streamlit app
â”œâ”€â”€ data/ # Folder containing PDF documents
â”œâ”€â”€ .env # Environment file for API keys
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You're here! 

---

## ğŸ§ª Sample Use Case

> You can ask:  
> - "What are the symptoms of stroke?"  
> - "How is stroke diagnosed?"  
> - "What treatment options exist?"  

The AI will search the uploaded stroke-related documents and provide relevant, conversational responses.

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/adubrightkwartengsnr/health-retrieval-augmented-generation.git
cd health-retrieval-augmented-generation

2. Create and activate a virtual environment
conda create -n rag-env python=3.10 -y
conda activate rag-env


3. Install dependencies
pip install -r requirements.txt

4. Add your API key
Create a .env file in the root directory:
GROQ_API_KEY=your_groq_api_key_here
ğŸ” Sign up for a Groq API key at https://console.groq.com/

5. Add your documents
Place relevant PDF files (e.g., stroke-related medical literature) in the data/ folder.

ğŸƒ Run the App
streamlit run app.py
Open http://localhost:8501 in your browser.

ğŸ› ï¸ Tech Stack
Tool |	Purpose
Streamlit |	UI  for real-time Q&A
LangChain |	Memory and document QA chain
FAISS|	Fast vector similarity search
HuggingFace| Embeddings	Convert text into embeddings
Groq| LLaMA 3 API	Powerful LLM backend
PyPDFLoader|	Parse and load PDF documents


ğŸ§  How It Works
PDFs are loaded and split into text chunks.
Chunks are embedded using sentence-transformers/all-MiniLM-L6-v2.
FAISS indexes the chunks for fast retrieval.
A conversational chain retrieves top chunks and queries the LLaMA model.
User conversation is tracked using ConversationBufferMemory.

ğŸ› Known Issues
If running on lower-spec machines, embeddings and FAISS may take time to load.
Warnings related to torch.classes during startup can be ignored (Streamlit internal behavior).
Replace .run() with .invoke() in the LangChain chain to avoid deprecation warnings.


ğŸ“Œ To-Do / Future Work
â¬œ Enable document upload from the UI
â¬œ Add multi-turn memory persistence (beyond session state)
â¬œ Deploy on Streamlit Cloud or Hugging Face Spaces

ğŸ“œ License
MIT License. Feel free to use, modify, and share.

ğŸ™‹â€â™‚ï¸ Contact
Developed by Bright Kwarteng Senior Adu
ğŸ“§ [adubrightkwarrteng11@gmail.com]
ğŸ”— https://www.linkedin.com/in/bright-adu-kwarteng-snr/

